{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38664bit94c2b907b01b48b59fac9ce7eb4999d1",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Speakers Recognition/Diarization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn.cluster\n",
    "from pyAudioAnalysis.MidTermFeatures import mid_feature_extraction as mT\n",
    "from pyAudioAnalysis.audioBasicIO import read_audio_file, stereo_to_mono\n",
    "from pyAudioAnalysis.audioSegmentation import labels_to_segments\n",
    "from pyAudioAnalysis.audioTrainTest import normalize_features\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wavfile\n",
    "import IPython"
   ]
  },
  {
   "source": [
    "#### Read and Normalize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read signal and get normalized segment feature statistics:\n"
   ]
  },
  {
   "source": [
    "#### Perform clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "#### Save clusters to concatenated wav files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speaker 0: 74 segments 471.3 sec total dur\n",
      "speaker 1: 69 segments 511.7 sec total dur\n"
     ]
    }
   ],
   "source": [
    "input_file = \"../../../../Audio/usable/400_AUDIO.wav\"\n",
    "fs, x = wavfile.read(input_file)\n",
    "mt_size, mt_step, st_win = 2, 0.1, 0.05\n",
    "[mt_feats, st_feats, _] = mT(x, fs, mt_size * fs, mt_step * fs,\n",
    "                            round(fs * st_win), round(fs * st_win * 0.5))\n",
    "(mt_feats_norm, MEAN, STD) = normalize_features([mt_feats.T])\n",
    "mt_feats_norm = mt_feats_norm[0].T\n",
    "\n",
    "n_clusters = 2\n",
    "x_clusters = [np.zeros((fs, )) for i in range(n_clusters)]\n",
    "k_means = sklearn.cluster.KMeans(n_clusters=n_clusters)\n",
    "k_means.fit(mt_feats_norm.T)\n",
    "cls = k_means.labels_\n",
    "\n",
    "segs, c = labels_to_segments(cls, mt_step)  # convert flags to segment limits\n",
    "for sp in range(n_clusters):                \n",
    "    count_cl = 0\n",
    "    for i in range(len(c)):     # for each segment in each cluster (>2 secs long)\n",
    "        if c[i] == sp and segs[i, 1]-segs[i, 0] > 2:\n",
    "            count_cl += 1\n",
    "            # get the signal and append it to the cluster's signal (followed by some silence)\n",
    "            cur_x = x[int(segs[i, 0] * fs): int(segs[i, 1] * fs)]\n",
    "            x_clusters[sp] = np.append(x_clusters[sp], cur_x)\n",
    "            x_clusters[sp] = np.append(x_clusters[sp], np.zeros((fs,)))\n",
    "    # write cluster's signal into a WAV file\n",
    "    print(f'speaker {sp}: {count_cl} segments {len(x_clusters[sp])/float(fs)} sec total dur')        \n",
    "    wavfile.write(f'../../../../Audio/processed_2/400_AUDIO_p_sd_{sp}.wav', fs, np.int16(x_clusters[sp]))\n",
    "    #IPython.display.display(IPython.display.Audio(f'diarization_cluster_{sp}.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}