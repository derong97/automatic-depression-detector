{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pronoun.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ncTfJrOLoFpS"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ5YUNoro8gI"
      },
      "source": [
        "# Importing necessary modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO2lXWmrGIRI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLNq0B3ng3wB",
        "outputId": "f27692d7-e456-41cf-bb80-7434778028e7"
      },
      "source": [
        "!pip install contractions\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import contractions\n",
        "import re as re\n",
        "import string\n",
        "import random  \n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Embedding, Flatten \n",
        "from keras.datasets import imdb \n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.vis_utils import plot_model\n",
        "# Metrics\n",
        "from sklearn import utils\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, \\\n",
        "        f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.base import clone\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "# Classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/ad/d1c685967945a04f8596128b15a1ab56c51488f53312e953341af6ff22d1/contractions-0.0.43-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.9MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 33.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81692 sha256=b0cc4d077b45ec5cdbb7bc20de73e1a4505e2524a83a40f7b7fab34367dd5f2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.43 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YQWjVhppB-n"
      },
      "source": [
        "# Removing stop words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saCL9NJehtfv"
      },
      "source": [
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# depressed persons tend to use first-person pronouns more, and third-person pronouns less. these words might provide indication\n",
        "excluded_pronouns = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
        "                    \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
        "                     'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']\n",
        "\n",
        "for pronoun in excluded_pronouns:\n",
        "    stop_words.remove(pronoun)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9irCgfKkH_s",
        "outputId": "010d143b-08c2-41d0-e0d3-825bb413b4df"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md                              raw_compiled_transcripts.csv\r\n",
            "pronoun.ipynb                          test_split_Depression_AVEC2017 (3).csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "6XIO4Ib6gPKI",
        "outputId": "e8060784-d4dc-4b16-c2cd-b4ef07461a8a"
      },
      "source": [
        "transcript_filepath = \"raw_compiled_transcripts.csv\"\n",
        "df_transcript = pd.read_csv(transcript_filepath, index_col = \"Participant_ID\")\n",
        "\n",
        "column_list = df_transcript.columns.tolist()\n",
        "print(column_list)\n",
        "\n",
        "df_transcript.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Transcript', 'PHQ_Score', 'PHQ_Binary']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Transcript</th>\n",
              "      <th>PHQ_Score</th>\n",
              "      <th>PHQ_Binary</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Participant_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>good atlanta georgia um my parents are from he...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>thank you mmm k i'm doing good thank you i'm f...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>i'm fine how about yourself  i'm from los ange...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>okay how 'bout yourself here in california yea...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>i'm doing good um from los angeles california ...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>i'm doing alright uh originally i'm from calif...</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>fine uh colorado mhm uh career career possibil...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>&lt;laughter&gt; um moscow um my family moved to the...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>los angeles california yes um the southern cal...</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>&lt;laughter&gt; &lt;laughter&gt; yeah &lt;laughter&gt; &lt;laughte...</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       Transcript  PHQ_Score  \\\n",
              "Participant_ID                                                                 \n",
              "300             good atlanta georgia um my parents are from he...          2   \n",
              "301             thank you mmm k i'm doing good thank you i'm f...          3   \n",
              "302             i'm fine how about yourself  i'm from los ange...          4   \n",
              "303             okay how 'bout yourself here in california yea...          0   \n",
              "304             i'm doing good um from los angeles california ...          6   \n",
              "305             i'm doing alright uh originally i'm from calif...          7   \n",
              "306             fine uh colorado mhm uh career career possibil...          0   \n",
              "307             <laughter> um moscow um my family moved to the...          4   \n",
              "308             los angeles california yes um the southern cal...         22   \n",
              "309             <laughter> <laughter> yeah <laughter> <laughte...         15   \n",
              "\n",
              "                PHQ_Binary  \n",
              "Participant_ID              \n",
              "300                      0  \n",
              "301                      0  \n",
              "302                      0  \n",
              "303                      0  \n",
              "304                      0  \n",
              "305                      0  \n",
              "306                      0  \n",
              "307                      0  \n",
              "308                      1  \n",
              "309                      1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKV7gZJdhRWT"
      },
      "source": [
        "def filter_text(text):\n",
        "    \n",
        "    # text = re.sub('<[^<]+?>', '', text) # remove anything enclosed by tags e.g. <>\n",
        "    # text = re.sub('\\[(.*?)\\]', '', text) # remove anything enclosed by closed brackets i.e. []\n",
        "    text = contractions.fix(text) # expands contractions e.g.'he's happy' -> 'he is happy'\n",
        "    \n",
        "    text = text.lower() # contractions will capitalize 'i'\n",
        "    # text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # remove remaining punctuations e.g. apostrophe\n",
        "    \n",
        "    tokens = TweetTokenizer().tokenize(text) # use TweetTokenizer as transcripts contain informal texts\n",
        "    \n",
        "    filtered_sentence = [w for w in tokens if w not in stop_words]\n",
        "    text = ' '.join(filtered_sentence)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "zZL05GFUhjcX",
        "outputId": "091530db-caba-4265-e031-48e44a6e9ab6"
      },
      "source": [
        "# Filter the text\n",
        "\n",
        "original_text = df_transcript.Transcript.iloc[30]\n",
        "\n",
        "df_transcript.Transcript = df_transcript.Transcript.apply(filter_text)\n",
        "filtered_text = df_transcript.Transcript.iloc[30]\n",
        "\n",
        "print(f\"The original text is:\\n{original_text}\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"The filtered text is:\\n{filtered_text}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original text is:\n",
            "<laughter> yes i'm okay with this very good l_a it's sunny and uh it's near the ocean <sigh> uh the traffic can make it difficult to drive around between different places and there's a lack of uh uh open uh spaces if you're not near a beach uh no um i uh i haven't planned on haven't finalized any travel plans listen to music yeah um uh well i tend to excel at things that  have to do with um uh spending time alone versus things that have to do with uh spending time outside with uh lots of people i'm reasonable mm maybe maybe yesterday i was trying to figure out how to stuff <stu> pack something into the car i'm very good at it <deep breath> i breathe in and out <sigh> like that <sigh> uh like jogging walking bicycling reading <sigh> listening to music uh math engineering computers uh a little bit yeah um i'm a student i'm going back to school for uh biological sciences um <sigh> started doing some reading on um some textbooks dream job um <sigh>  medical doctor uh it's i guess it depends on the type of medicine you practice <laughter> uh i have a mentor yeah i have a mentor who has a lot of positive um thoughts and um suggestions i uh through a professional so i met someone through a class and then they worked with them professionally so it's i guess you could say through an academic or professional channel <sigh> hmm mm <sigh> uh let's see one of my most memorable i can't think of one at the moment no no i don't think so no <sigh> takes me a little while to fall asleep but once i'm asleep it's it's reasonably okay uh i think i'm a little forgetful and nervous no sometimes mm what do you wanna know mm <sigh> well um i think sometimes i do think about  challenges ahead of me and uh that can be a little daunting and i i feel down it is very hard go for a walk read a book listen to some music um <sigh> i don't i mean i'm sure they are i'm not specifically sure what triggers anything uh maybe feeling overwhelmed or uh not feeling particularly tied to some task i don't go to a therapist um reasonably smart inquisitive <inqui> inquisitive and curious mm i guess through classes uh <sigh> somewhat mm patience and curiosity uh <sigh> enjoyed hmm mm think i went on a walk at night uh a day ago no can you repeat that can you rephrase that uh i think they're lucky very lucky got everything in front of 'em yeah uh <sigh> most proud of <deep breath> <sigh> maybe academic achievements uh i guess just the time i've spent studying math and science um hmm mm mm i'm not sure\n",
            "-------------------------------------------\n",
            "The filtered text is:\n",
            "<laughter> yes i okay good l_a it sunny uh it near ocean <sigh> uh traffic make it difficult drive around different places lack uh uh open uh spaces you near beach uh um i uh i planned finalized travel plans listen music yeah um uh well i tend excel things um uh spending time alone versus things uh spending time outside uh lots people i reasonable mm maybe maybe yesterday i trying figure stuff <stu> pack something car i good it < deep breath > i breathe <sigh> like <sigh> uh like jogging walking bicycling reading <sigh> listening music uh math engineering computers uh little bit yeah um i student i going back school uh biological sciences um <sigh> started reading um textbooks dream job um <sigh> medical doctor uh it i guess it depends type medicine you practice <laughter> uh i mentor yeah i mentor lot positive um thoughts um suggestions i uh professional i met someone class they worked them professionally it i guess you could say academic professional channel <sigh> hmm mm <sigh> uh let us see one my memorable i think one moment i think <sigh> takes me little fall asleep i asleep it it reasonably okay uh i think i little forgetful nervous sometimes mm you want know mm <sigh> well um i think sometimes i think challenges ahead me uh little daunting i i feel it hard go walk read book listen music um <sigh> i i mean i sure they i specifically sure triggers anything uh maybe feeling overwhelmed uh feeling particularly tied task i go therapist um reasonably smart inquisitive <inqui> inquisitive curious mm i guess classes uh <sigh> somewhat mm patience curiosity uh <sigh> enjoyed hmm mm think i went walk night uh day ago you repeat you rephrase uh i think they lucky lucky got everything front them yeah uh <sigh> proud < deep breath > <sigh> maybe academic achievements uh i guess time i spent studying math science um hmm mm mm i sure\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcjth9ktraLl"
      },
      "source": [
        "# Counting different features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqCRuFpkym3Z"
      },
      "source": [
        "## Tagged\n",
        "We need to find actions that are tagged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-7SRt9urXp8"
      },
      "source": [
        "import re\n",
        "\n",
        "def find_angle_bracket(transcript): \n",
        "  # for word in transcript:\n",
        "    # print(word)\n",
        "    pattern = \"<(.*?)>\"\n",
        "\n",
        "    substring = re.search(pattern, transcript)\n",
        "\n",
        "    approved = [\"laughter\", \"clears throat\", \"sigh\" ,\"yawn\", \"sniff\", \"sniffle\",\n",
        "                \"cough\", \"sharp inhale\", \"sharp inhaling\", \"sneeze\", \"deep breath\"]\n",
        "\n",
        "    if substring == None:\n",
        "      return \"None\"\n",
        "    else: \n",
        "      x = substring.group(1).strip() \n",
        "      if x not in approved:\n",
        "        return \"None\"\n",
        "      else:     \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JcPcZ-Gyr1Z"
      },
      "source": [
        "def find_square_bracket(transcript): \n",
        "  pattern = \"\\[(.*?)\\]\"\n",
        "\n",
        "  substring = re.search(pattern, transcript)\n",
        "\n",
        "  #remove all spaces so all input is constant\n",
        "  approved = [\"laughter\", \"clears throat\", \"sigh\" ,\"yawn\", \"sniff\", \"sniffle\",\n",
        "              \"cough\", \"sharp inhale\", \"sharp inhaling\", \"sneeze\", \"deep breath\"]\n",
        "\n",
        "  if substring == None:\n",
        "    return \"None\"\n",
        "  else: \n",
        "    x = substring.group(1).strip() \n",
        "    # print(x)\n",
        "    if x not in approved:\n",
        "      return \"None\"\n",
        "    else:     \n",
        "      return (x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWa75_9aoCJh"
      },
      "source": [
        "## Pronouns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4q_41b9cIbG"
      },
      "source": [
        "def calculating_score(text, target_words):\n",
        "    score = 0\n",
        "    for w in text:\n",
        "        for x in target_words:\n",
        "            if w == x:\n",
        "                score += 1\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncTfJrOLoFpS"
      },
      "source": [
        "## Putting Everything Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTBuDkO2afd",
        "outputId": "4b5699da-71ea-4f9c-9076-c6b071d077d5"
      },
      "source": [
        "# iterate through raw_compiled_scripts and add count \n",
        "\n",
        "def count_features():\n",
        "\n",
        "  # creating new dataframe output \n",
        "  columns = [\"Participant_ID\", \"Laughs\", \"Sighs\", \"Clears_Throat\", \"Yawns\", \"Sniffs\", \"Coughs\", \"Sharp_Inhales\", \"Sneezes\", \"Deep_Breaths\", \n",
        "             \"Absolute_Words\", \"First_Pronoun_Singular\", \"First_Pronoun_Plural\", \"Third_Pronoun\"]\n",
        "  df_features = pd.DataFrame(columns=columns)\n",
        "  # df_features.head()  \n",
        "\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  absolute_list = [\"complet\", \"never\", \"absolut\", \"onli\", \"everi\", \"not\" \"alway\", \"just\", \"none\"]\n",
        "  first_pronoun_s_list = [\"i\", \"me\", \"mine\", \"myself\"]\n",
        "  first_pronoun_p_list = [\"we\", \"us\"]\n",
        "  third_pronoun_list = [\"he\", \"she\", \"it\", \"they\", \"him\", \"her\", \"it\", \"your\", \"his\", \"her\"]\n",
        "\n",
        "  #iterate through all words in transcript and count features \n",
        "  for index, row in df_transcript.iterrows():\n",
        "      participant_ID = index\n",
        "      phq_Binary = row[\"PHQ_Binary\"]\n",
        "      transcript = row[\"Transcript\"]\n",
        "\n",
        "      transcript = transcript.split()\n",
        "\n",
        "      if index == 300:\n",
        "        print(transcript)\n",
        "\n",
        "      if index == 333:\n",
        "        print(transcript)\n",
        "    \n",
        "      laughs = 0\n",
        "      sighs = 0 \n",
        "      clears_throat = 0 \n",
        "      yawns = 0 \n",
        "      sniffs = 0\n",
        "      coughs = 0\n",
        "      sharp_inhales = 0 \n",
        "      sneezes = 0\n",
        "      deep_breaths = 0 \n",
        "\n",
        "      absolute_words = 0\n",
        "      first_pronouns_s = 0\n",
        "      first_pronouns_p = 0\n",
        "      third_pronouns = 0\n",
        "\n",
        "      #calculating pronouns:\n",
        "      absolute_words = calculating_score(transcript, absolute_list)\n",
        "      first_pronouns_s = calculating_score(transcript, first_pronoun_s_list)\n",
        "      first_pronouns_p = calculating_score(transcript, first_pronoun_p_list)\n",
        "      third_pronouns = calculating_score(transcript, third_pronoun_list)\n",
        "\n",
        "      for word in transcript: \n",
        "        word_angle = find_angle_bracket(word)\n",
        "        word_square = find_square_bracket(word) \n",
        "\n",
        "        # get how many actions 1 person did\n",
        "        if word_angle != \"None\" and word_square == \"None\":\n",
        "          if word_angle == \"laughter\":\n",
        "            laughs += 1\n",
        "          elif word_angle == \"clears throat\":\n",
        "            clears_throat += 1\n",
        "          elif word_angle == \"sigh\": \n",
        "            sighs += 1\n",
        "          elif word_angle == \"yawn\":\n",
        "            yawns += 1\n",
        "          elif word_angle == \"sniff\":\n",
        "            sniffs += 1\n",
        "          elif word_angle == \"sniffle\":\n",
        "            sniffs += 1\n",
        "          elif word_angle == \"cough\":\n",
        "            coughs += 1\n",
        "          elif word_angle == \"sharp inhale\":\n",
        "            sharp_inhales += 1\n",
        "          elif word_angle == \"sharp inhaling\":\n",
        "            sharp_inhales += 1\n",
        "          elif word_angle == \"sneeze\":\n",
        "            sneezes += 1\n",
        "          elif word_angle == \"deep breath\":\n",
        "            deep_breaths += 1\n",
        "        \n",
        "        elif word_square != \"None\" and word_angle == \"None\":\n",
        "          if word_square == \"laughter\":\n",
        "            laughs += 1\n",
        "          elif word_square == \"clears throat\":\n",
        "            clears_throat += 1\n",
        "          elif word_square == \"sigh\": \n",
        "            sighs += 1\n",
        "          elif word_square == \"yawn\":\n",
        "            yawns += 1\n",
        "          elif word_square == \"sniff\":\n",
        "            sniffs += 1\n",
        "          elif word_square == \"sniffle\":\n",
        "            sniffs += 1\n",
        "          elif word_square == \"cough\":\n",
        "            coughs += 1\n",
        "          elif word_square == \"sharp inhale\":\n",
        "            sharp_inhales += 1\n",
        "          elif word_square == \"sharp inhaling\":\n",
        "            sharp_inhales += 1\n",
        "          elif word_square == \"sneeze\":\n",
        "            sneezes += 1\n",
        "          elif word_square == \"deep breath\":\n",
        "            deep_breaths += 1\n",
        "\n",
        "        # counting the number of pronouns \n",
        "\n",
        "        stemmed = ps.stem(word)\n",
        "\n",
        "        # if stemmed in absolute_list: \n",
        "        \n",
        "        # if stemmed in first_pronoun_s_list:\n",
        "        #   first_pronouns_s += 1\n",
        "        # elif stemmed in first_pronoun_p_list:\n",
        "        #   first_pronouns_p += 1\n",
        "        # elif stemmed in third_pronoun: \n",
        "        #   third_pronouns += 1\n",
        "    \n",
        "      new_row = {\n",
        "                \"Participant_ID\": index,\n",
        "                \"Laughs\": laughs,\n",
        "                \"Sighs\": sighs,\n",
        "                \"Clears_Throat\": clears_throat,\n",
        "                \"Yawns\": yawns,\n",
        "                \"Sniffs\": sniffs,\n",
        "                \"Coughs\": coughs,\n",
        "                \"Sharp_Inhales\": sharp_inhales,\n",
        "                \"Sneezes\": sneezes,\n",
        "                \"Deep_Breaths\": deep_breaths,\n",
        "                \"Absolute_Words\": absolute_words,\n",
        "                \"First_Pronoun_Singular\": first_pronouns_s,\n",
        "                \"First_Pronoun_Plural\": first_pronouns_p,\n",
        "                \"Third_Pronoun\": third_pronouns,\n",
        "                # \"PHQ_Binary\": phq_Binary\n",
        "      }\n",
        "\n",
        "      # print(new_row)\n",
        "\n",
        "      df_features = df_features.append(new_row, ignore_index = True)\n",
        "  \n",
        "  return df_features\n",
        "\n",
        "df_features = count_features() \n",
        "print(df_features.columns.tolist())\n",
        "df_features.head(30)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['good', 'atlanta', 'georgia', 'um', 'my', 'parents', 'um', 'i', 'love', 'it', 'i', 'like', 'weather', 'i', 'like', 'opportunities', 'um', 'yes', 'um', 'it', 'took', 'minute', 'somewhat', 'easy', 'congestion', 'it', 'um', 'i', 'took', 'business', 'administration', 'uh', 'yeah', 'i', 'i', 'break', 'right', 'i', 'plan', 'going', 'back', 'uh', 'next', 'semester', 'uh', 'probably', 'open', 'my', 'business', 'um', 'specific', 'reason', 'i', 'travel', 'lot', 'i', 'pretty', 'local', 'year', 'you', 'little', 'bit', 'specific', 'answer', 'i', 'like', 'reading', 'books', 'i', 'enjoy', 'i', 'enjoy', 'cooking', 'um', 'exercising', 'great', 'i', 'i', 'i', 'pretty', 'good', 'it', 'um', 'yeah', 'um', 'probably', 'two', 'weeks', 'ago', 'uh', 'frustrated', 'um', 'i', 'like', 'bias', 'um', 'i', 'like', 'um', 'someone', 'says', 'they', 'going', 'something', 'they', 'uh', 'somewhat', 'friendship', 'i', 'like', 'play', 'sports', 'i', 'enjoy', 'uh', 'going', 'friends', 'family', 'playing', 'games', 'grandparents', 'parents', 'um', 'yeah', 'i', 'mean', 'they', 'always', 'given', 'me', 'great', 'advice', 'they', 'always', 'kept', 'it', 'real', 'real', 'close', 'i', 'would', 'say', 'going', 'college', 'right', 'high', 'school', 'well', 'i', 'would', 'done', 'you', 'know', 'i', 'would', 'probably', 'field', 'career', 'field', 'uh', 'taking', 'job', 'street', 'i', 'sure', 'i', 'could', 'yes', 'i', 'sure', 'maybe', 'i', 'graduated', 'high', 'school', 'well', 'uh', 'i', 'um', 'i', 'got', 'my', 'diploma', 'my', 'my', 'diploma', 'i', 'finished', 'school', 'i', 'met', 'requirements', 'high', 'school', 'i', 'approved', 'go', 'whatever', 'i', 'wanted', 'living', 'um', 'it', 'alright', 'it', 'could', 'better', 'uh', 'it', 'pretty', 'easy', 'uh', 'yes', 'repeat', 'irritated', 'um', 'lazy', 'um', 'day', 'weather', 'great', 'sun', 'different', 'less', 'less', 'um', 'interested', 'uh', 'shut', 'uh', 'two', 'weeks', 'ago', 'uh', 'yeah', 'friend', 'mine', 'annoying', 'me', 'i', 'cut', 'them', '[', 'laughter', ']', 'it', 'alright', 'friendship', 'chocolate', 'tall', 'thin', 'thank', 'you', 'bye', 'bye']\n",
            "['yes', 'well', 'new', 'york', 'turn', 'volume', 'little', 'bit', '<laughter>', 'uh', 'change', 'yeah', 'it', 'good', 'i', 'wanted', '<wa>', 'i', 'wanted', 'i', 'wanted', 'see', 'los', 'angeles', 'um', 'probably', 'took', 'year', 'two', 'uh', 'you', 'know', 'away', 'my', 'family', 'away', 'i', 'used', 'new', 'york', 'neighborhoods', 'people', 'getting', 'acclimated', 'whole', 'new', 'lifestyle', 'uh', 'weather', 'you', 'beat', 'weather', 'it', 'generally', 'always', 'nice', 'it', 'hot', 'it', 'cold', 'uh', 'it', 'creative', 'i', 'like', 'uh', 'you', 'know', 'movies', 'uh', 'like', 'beach', 'traffic', 'it', 'lot', 'traffic', 'uh', 'sometimes', 'i', 'think', 'people', 'little', 'bit', 'colder', 'distant', 'unfriendly', 'places', 'i', 'history', 'major', 'uh', 'i', 'uh', 'demonstrations', 'really', '<rea>', 'uh', 'high-end', 'supermarkets', 'uh', 'screenwriter', 'one', 'reasons', 'i', 'came', 'los', 'angeles', 'um', 'it', 'hard', 'i', 'always', 'maintain', 'uh', 'focus', 'motivation', 'i', 'let', 'things', 'distract', 'me', '<laughter>', 'uh', 'i', 'uh', 'i', 'always', 'interested', 'films', 'ever', 'since', 'i', 'child', 'i', 'always', 'you', 'know', 'writing', 'active', 'imagination', 'uh', 'i', 'last', 'probably', 'almost', 'ten', 'years', 'i', 'traveled', 'i', 'past', '<pa>', 'well', 'i', 'always', 'traveled', 'even', 'kid', 'my', 'parents', 'would', 'travel', 'around', 'yes', 'seeing', 'new', 'places', 'seeing', 'uh', 'people', 'different', 'i', 'seeing', 'they', 'live', 'taking', 'sights', 'uh', 'i', 'went', 'kenya', 'probably', 'one', 'best', 'trips', 'i', 'ever', 'went', 'um', 'safari', 'going', 'jeep', 'seeing', '<laughter>', 'wild', 'animals', 'their', 'natural', 'habitat', 'pretty', 'incredible', 'oh', 'yeah', 'one', 'great', 'experiences', 'my', 'life', 'i', 'also', 'got', 'uh', 'place', 'egypt', 'uh', 'end', 'trip', 'um', 'called', 'uh', 'sharm', 'el', 'sheikh', 'right', 'water', 'it', 'best', 'scuba', 'diving', 'world', 'hard', 'believe', 'egypt', 'would', 'it', 'amazing', 'i', 'went', '<we>', 'i', 'go', 'scuba', 'diving', 'i', 'went', 'uh', 'snorkeling', 'i', 'relax', 'uh', 'i', 'i', 'i', 'like', 'listen', 'music', 'i', 'like', 'uh', 'helping', 'watching', 'movies', 'helps', 'me', 'relax', 'it', 'takes', 'me', 'you', 'know', 'whatever', 'might', 'my', 'mind', 'i', 'meditate', 'every', 'day', 'um', 'i', 'like', 'take', 'walks', 'nature', 'always', 'calming', 'especially', 'uh', 'ocean', 'uh', 'i', 'it', 'depends', '<laughter>', 'sometimes', 'i', 'get', 'angry', 'lose', 'my', 'temper', 'uh', 'lot', 'times', 'uh', 'i', 'like', 'form', 'self-hypnosis', 'calm', 'myself', 'something', 'i', 'recently', 'learned', 'helpful', 'also', 'since', 'i', 'meditate', 'it', 'i', 'get', 'annoyed', 'things', 'easily', 'i', 'past', 'things', 'makes', 'me', 'really', 'mad', 'i', 'guess', 'uh', 'think', 'injustice', 'world', 'done', 'people', 'uh', 'it', 'really', 'need', 'way', 'we', 'could', 'get', 'along', 'um', 'make', 'world', 'better', '<laughter>', 'uh', 'last', 'night', 'i', 'went', 'movie', 'screening', 'independent', 'film', 'i', 'think', 'it', 'done', 'well', 'it', 'much', 'we', 'fun', 'argument', 'uh', 'movie', 'like', 'you', 'know', 'happened', 'yesterday', 'my', 'mind', '<laughter>', 'anything', 'i', 'regret', 'yeah', 'opportunities', 'my', 'life', 'i', 'wished', 'i', 'uh', 'taken', 'them', 'pursued', 'them', 'uh', 'certain', 'jobs', 'you', 'know', 'i', 'first', 'got', 'los', 'angeles', 'i', 'opportunity', 'uh', 'become', 'grip', 'uh', 'i', 'i', 'probably', 'pursued', 'i', 'decide', 'it', 'i', 'really', 'understand', 'questions', 'take', 'advantage', 'every', 'opportunity', 'came', 'my', 'way', 'always', '<alway>', 'i', 'think', 'saying', 'yes', 'uh', 'opportunity', 'important', 'matter', 'you', 'feel', 'you', 'know', 'you', 'confident', 'you', 'able', 'achieve', 'you', 'want', 'i', 'depressed', 'my', 'life', 'uh', 'part', 'i', 'i', 'feeling', 'good', 'sometimes', 'uh', 'i', 'feel', 'little', 'blue', 'uh', 'i', 'think', 'you', 'know', 'sometimes', 'i', 'i', 'allow', '<allo>', 'allow', 'regret', 'come', 'my', 'mind', 'bring', 'me', 'i', 'you', 'know', 'usually', 'get', 'it', 'pretty', 'quickly', 'uh', 'sometimes', 'financial', 'concerns', 'sometimes', 'uh', 'opportunity', 'um', 'i', 'explain', 'it', 'eh', 'sometimes', '<so>', 'sometimes', '<someti>', 'eh', 'financial', 'one', 'way', 'i', 'i', 'i', 'made', 'another', 'decision', 'i', 'might', 'money', 'my', 'life', 'would', 'different', 'um', 'really', 'i', 'mean', 'i', 'i', 'feel', 'pretty', 'much', 'got', 'me', 'seek', 'help', '<laughter>', 'let', 'us', 'come', 'back', 'um', 'i', 'well', 'okay', 'well', 'it', 'generally', 'talking', 'someone', 'helped', 'i', 'uh', 'pretty', 'easy', 'i', 'problem', 'sleeping', 'uh', 'usually', 'i', 'grumpy', '<laughter>', 'i', 'sleep', 'well', 'uh', 'you', 'know', 'i', 'feel', 'tired', 'irritable', 'uh', 'would', 'my', 'best', 'friend', 'describe', 'me', 'good', 'question', 'um', 'friendly', '<f>', 'well', 'i', 'friendly', '<f>', 'i', 'definitely', 'friendly', 'outgoing', 'uh', 'little', 'bit', 'secretive', 'um', 'i', 'would', 'probably', 'i', 'they', 'would', 'like', 'see', 'me', 'accomplished', 'uh', 'uh', 'attaining', 'my', 'goals', 'happier', '<ha>', 'maybe', 'little', 'happier', 'i', 'think', 'um', 'it', 'it', 'it', 'tough', 'it', 'depends', 'sometimes', 'i', 'introverted', 'i', 'like', 'write', 'takes', 'it', 'means', 'i', 'spend', 'time', 'alone', 'i', 'come', 'uh', 'i', 'extroverted', 'i', 'really', 'see', 'myself', 'one', 'hard', 'way', 'it', 'depends', 'my', 'situation', 'my', 'mood', 'my', 'best', 'qualities', 'um', 'i', 'think', 'i', 'caring', 'i', 'think', 'i', 'intelligent', 'uh', 'i', 'think', 'i', 'funny', 'i', 'outgoing', 'um', 'i', 'likeable', 'personality', 'you', 'know', 'people', 'said', 'um', 'i', 'engaging', 'uh', 'oppressive', 'overly', 'engaging', 'sort', 'cool', 'detachment', 'um', 'creative', 'intuitive', '<laughter>', 'yes', '<laughter>', 'last', 'time', 'i', 'felt', 'really', 'happy', 'i', 'know', 'i', 'woke', 'morning', 'i', 'pretty', 'happy', 'i', 'breakfast', 'i', 'happy', 'sun', 'shining', '<laughter>', 'um', 'i', 'think', 'like', 'i', 'said', 'going', 'kenya', 'memorable', 'experience', 'i', 'remember', '<remem>', 'scuba', 'uh', 'snorkeling', 'really', 'great', 'experience', '<ex>', 'experience', 'i', 'remembered', 'you', 'know', 'seeing', 'colors', 'ocean', 'you', 'know', 'feeling', 'sun', 'powerful', 'sun', 'my', 'back', 'um', 'almost', 'beyond', 'you', 'know', '<beca>', 'almost', 'beyond', 'like', 'human', 'way', 'touch', 'like', 'everything', 'universe', 'yeah', 'it', 'would', 'nice', 'feel', 'way', 'every', 'moment', 'day', 'i', 'proud', 'my', 'life', 'i', 'think', 'i', 'uh', 'helped', '<pft>', 'people', 'uh', 'sometimes', 'they', 'crisis', 'um', 'you', 'know', 'i', 'written', 'screenplays', '<scr>', 'screenplays', 'um', 'i', 'good', 'relationships', 'um', 'good', 'relationships', 'you', 'know', 'connected', 'someone', 'else', 'you', 'know', 'spending', 'time', 'them', 'connecting', 'emotional', '<laughter>', 'physical', 'mental', 'levels', 'um', 'it', 'could', 'song', 'i', 'hear', 'radio', 'uh', 'it', 'could', 'uh', 'lot', 'times', 'looking', 'forward', 'something', 'you', 'know', 'i', 'going', 'go', 'movie', 'go', 'bowling', 'hang', 'people', 'um', 'you', 'know', 'looking', 'work', 'going', 'work', 'feeling', 'productive', 'put', 'me', 'it', 'puts', 'me', 'good', 'mood', 'you', 'welcome', 'bye', 'bye']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "['Participant_ID', 'Laughs', 'Sighs', 'Clears_Throat', 'Yawns', 'Sniffs', 'Coughs', 'Sharp_Inhales', 'Sneezes', 'Deep_Breaths', 'Absolute_Words', 'First_Pronoun_Singular', 'First_Pronoun_Plural', 'Third_Pronoun']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>Laughs</th>\n",
              "      <th>Sighs</th>\n",
              "      <th>Clears_Throat</th>\n",
              "      <th>Yawns</th>\n",
              "      <th>Sniffs</th>\n",
              "      <th>Coughs</th>\n",
              "      <th>Sharp_Inhales</th>\n",
              "      <th>Sneezes</th>\n",
              "      <th>Deep_Breaths</th>\n",
              "      <th>Absolute_Words</th>\n",
              "      <th>First_Pronoun_Singular</th>\n",
              "      <th>First_Pronoun_Plural</th>\n",
              "      <th>Third_Pronoun</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>301</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>164</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>302</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>303</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>130</td>\n",
              "      <td>3</td>\n",
              "      <td>174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>304</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>83</td>\n",
              "      <td>7</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>305</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>326</td>\n",
              "      <td>8</td>\n",
              "      <td>204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>306</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>77</td>\n",
              "      <td>6</td>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>307</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>28</td>\n",
              "      <td>164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>308</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>309</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>310</td>\n",
              "      <td>31</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>311</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>312</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>98</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>313</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>3</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>314</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>210</td>\n",
              "      <td>37</td>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>315</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>119</td>\n",
              "      <td>7</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>316</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>317</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>318</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "      <td>4</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>319</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>2</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>320</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>3</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>321</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>108</td>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>322</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>118</td>\n",
              "      <td>6</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>323</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>102</td>\n",
              "      <td>36</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>324</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>325</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>174</td>\n",
              "      <td>9</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>326</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>327</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>328</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>127</td>\n",
              "      <td>27</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>329</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>3</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Participant_ID Laughs Sighs Clears_Throat Yawns Sniffs Coughs  \\\n",
              "0             300      0     0             0     0      0      0   \n",
              "1             301      3     0             0     0      0      0   \n",
              "2             302      1     0             0     0      0      0   \n",
              "3             303      3     0             0     0      0      0   \n",
              "4             304     11     0             0     0      0      0   \n",
              "5             305      3     0             0     0      1      0   \n",
              "6             306      0     0             0     0      0      0   \n",
              "7             307      1     0             0     0      0      0   \n",
              "8             308      1     5             0     1      1      0   \n",
              "9             309     13     0             0     0      0      0   \n",
              "10            310     31     6             0     0      0      1   \n",
              "11            311      8     3             0     0      1      0   \n",
              "12            312      0     2             0     0      0      0   \n",
              "13            313      2     0             0     0      1      1   \n",
              "14            314      0     0             0     0      0      0   \n",
              "15            315      2     7             0     0      0      0   \n",
              "16            316      4     5             0     0      0      0   \n",
              "17            317      4     0             0     0      0      3   \n",
              "18            318     11     2             0     0      0      0   \n",
              "19            319      1     3             0     0      3      2   \n",
              "20            320      1     0             0     0      0      0   \n",
              "21            321      6     0             0     0      0      0   \n",
              "22            322      3     0             0     0      0      0   \n",
              "23            323      7     1             0     0      0      0   \n",
              "24            324      1     0             0     1      0      0   \n",
              "25            325      9     0             0     0      0      0   \n",
              "26            326      9     1             0     0      0      0   \n",
              "27            327      1     5             0     0      0      0   \n",
              "28            328     18     5             0     0      0      0   \n",
              "29            329      2     0             0     0      0      0   \n",
              "\n",
              "   Sharp_Inhales Sneezes Deep_Breaths Absolute_Words First_Pronoun_Singular  \\\n",
              "0              0       0            0              0                     37   \n",
              "1              0       0            0              2                    164   \n",
              "2              0       0            0              0                     29   \n",
              "3              0       0            0              5                    130   \n",
              "4              0       0            0              1                     83   \n",
              "5              0       0            0              1                    326   \n",
              "6              0       0            0              1                     77   \n",
              "7              0       0            0              2                    188   \n",
              "8              0       0            0              0                     87   \n",
              "9              0       0            0              1                     66   \n",
              "10             0       0            0              0                    111   \n",
              "11             0       0            0              0                     50   \n",
              "12             0       0            0              0                     98   \n",
              "13             0       0            0              0                     58   \n",
              "14             0       0            0              3                    210   \n",
              "15             0       0            0              0                    119   \n",
              "16             0       1            0              1                     58   \n",
              "17             0       0            0              1                     55   \n",
              "18             0       0            0              1                     51   \n",
              "19             0       0            0              0                     62   \n",
              "20             0       0            0              0                     94   \n",
              "21             0       0            0              1                    108   \n",
              "22             0       0            0              0                    118   \n",
              "23             0       0            0              2                    102   \n",
              "24             0       0            0              0                     74   \n",
              "25             0       0            0              0                    174   \n",
              "26             0       0            0              0                     34   \n",
              "27             0       0            0              0                     55   \n",
              "28             0       0            0              1                    127   \n",
              "29             0       0            0              0                     88   \n",
              "\n",
              "   First_Pronoun_Plural Third_Pronoun  \n",
              "0                     0            22  \n",
              "1                     0           125  \n",
              "2                     4            46  \n",
              "3                     3           174  \n",
              "4                     7            90  \n",
              "5                     8           204  \n",
              "6                     6           119  \n",
              "7                    28           164  \n",
              "8                     0            53  \n",
              "9                     0            34  \n",
              "10                    3            64  \n",
              "11                    0            41  \n",
              "12                    0            88  \n",
              "13                    3            29  \n",
              "14                   37           249  \n",
              "15                    7            94  \n",
              "16                    0            31  \n",
              "17                    2            22  \n",
              "18                    4            44  \n",
              "19                    2            36  \n",
              "20                    3            56  \n",
              "21                    3            60  \n",
              "22                    6            68  \n",
              "23                   36            49  \n",
              "24                    2            45  \n",
              "25                    9            92  \n",
              "26                    1            27  \n",
              "27                    1            56  \n",
              "28                   27           203  \n",
              "29                    3            46  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp7B1qBuoc84"
      },
      "source": [
        "# Creating Sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ebYeJ5Lohqy"
      },
      "source": [
        "## Normal Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7rqTuJ9gOlz",
        "outputId": "c104d210-bb08-4628-e174-f0b8f6eaafb1"
      },
      "source": [
        "np_features = df_features.to_numpy()\n",
        "print(np_features.shape)\n",
        "print(np_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(181, 14)\n",
            "[[300 0 0 ... 37 0 22]\n",
            " [301 3 0 ... 164 0 125]\n",
            " [302 1 0 ... 29 4 46]\n",
            " ...\n",
            " [490 1 3 ... 33 0 22]\n",
            " [491 4 3 ... 93 14 78]\n",
            " [492 8 0 ... 96 5 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHO4O8CKpcIW"
      },
      "source": [
        "def train_test(X, y, testfile):\n",
        "    test_participants = pd.read_csv(testfile)['participant_ID'].values\n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "    \n",
        "    for i in range(y.shape[0]):\n",
        "        participant_no = y.index[i]\n",
        "        \n",
        "        if participant_no in test_participants:\n",
        "            X_test.append(X[i])\n",
        "            y_test.append(y[participant_no])\n",
        "        \n",
        "        else:\n",
        "            X_train.append(X[i])\n",
        "            y_train.append(y[participant_no])\n",
        "            \n",
        "    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1xqxh8apfKo",
        "outputId": "cc00a7bb-d437-4af5-8695-8abd13d55e1b"
      },
      "source": [
        "testfile = './test_split_Depression_AVEC2017 (3).csv'\n",
        "\n",
        "X = np_features\n",
        "y =  pd.read_csv(\"./raw_compiled_transcripts.csv\", index_col = \"Participant_ID\")['PHQ_Binary']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test(X, y, testfile)\n",
        "\n",
        "np.save(\"X_train_p\", X_train)\n",
        "np.save(\"y_train_p\", y_train)\n",
        "np.save(\"X_test_p\", X_test)\n",
        "np.save(\"y_test_p\", y_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(136, 14)\n",
            "(45, 14)\n",
            "(136,)\n",
            "(45,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcEXb7WywCi4",
        "outputId": "50370677-44b2-43c6-f54a-ea49d6525125"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "X_train = np.load(\"X_train_p.npy\", allow_pickle = True)\n",
        "y_train = np.load(\"y_train_p.npy\",  allow_pickle = True)\n",
        "\n",
        "X_test = np.load(\"X_test_p.npy\",  allow_pickle = True)\n",
        "y_test = np.load(\"y_test_p.npy\",  allow_pickle = True)\n",
        "\n",
        "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(136, 13) (45, 13) (136,) (45,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79kwCiPRoi6T"
      },
      "source": [
        "## Undersample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xvLpoejhjrA"
      },
      "source": [
        "RANDOM_STATE = 42\n",
        "\n",
        "# because the RandomUnderSampler does not work here\n",
        "def undersampling(X_train, y_train):\n",
        "    random.seed(RANDOM_STATE)\n",
        "    \n",
        "    neg_list = [i for i in range(len(y_train)) if y_train[i] == 0]\n",
        "    pos_list = [i for i in range(len(y_train)) if y_train[i] == 1]\n",
        "    \n",
        "    if len(neg_list) < len(pos_list):\n",
        "        minority_list = neg_list\n",
        "        majority_list = pos_list\n",
        "    else:\n",
        "        minority_list = pos_list\n",
        "        majority_list = neg_list\n",
        "        \n",
        "    sampled_list = random.sample(majority_list, len(minority_list))\n",
        "    \n",
        "    final_list = sampled_list + minority_list\n",
        "    \n",
        "    X_train_us = []\n",
        "    y_train_us = []\n",
        "    \n",
        "    for i in final_list:\n",
        "        X_train_us.append(X_train[i])\n",
        "        y_train_us.append(y_train[i])\n",
        "                                 \n",
        "    return np.array(X_train_us), np.array(y_train_us)\n",
        "\n",
        "xu_train, yu_train = undersampling(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkQiOGfrhkdZ",
        "outputId": "66689f52-1b70-462e-81e6-a2dd6015e558"
      },
      "source": [
        "# shuffle the train data in unison because data is in order\n",
        "# reduces poor performance during k-cross validation when sampling data\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "xu_train, yu_train = shuffle(xu_train, yu_train, random_state=RANDOM_STATE)\n",
        "\n",
        "xu_train.shape, X_test.shape, yu_train.shape, y_test.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((82, 13), (45, 13), (82,), (45,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI-FqqBhj_R9"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OthD2Hy0kC8W"
      },
      "source": [
        "def evaluate_on_training_set(y_test, y_pred):\n",
        "    print(y_pred)\n",
        "    print(y_test)\n",
        "    \n",
        "    # Calculate AUC\n",
        "    print(\"AUC is: \", roc_auc_score(y_test, y_pred))\n",
        "\n",
        "    # print out recall and precision\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # print out confusion matrix\n",
        "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # # calculate points for ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_score(y_test, y_pred))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzg2QIaPkE61"
      },
      "source": [
        "def k_cross(input_model, X, y, k=4, n=3, random_state=42):\n",
        "    f1_scores = []\n",
        "    recall_scores = []\n",
        "    rkf = RepeatedKFold(n_splits=k, n_repeats=n, random_state=random_state)\n",
        "    for train_index, val_index in rkf.split(X):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "        \n",
        "        model = clone(input_model)\n",
        "        model.fit(X_train, y_train) \n",
        "        \n",
        "        y_pred = model.predict(X_val)\n",
        "        f1 = f1_score(y_val, y_pred, average='micro')\n",
        "        f1_scores.append(f1)\n",
        "        recall = recall_score(y_val, y_pred, average='micro')\n",
        "        recall_scores.append(recall)\n",
        "        \n",
        "    return f1_scores, recall_scores, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uDFPYUjkLr_"
      },
      "source": [
        "## Evaluate Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXMRztI2kOI_"
      },
      "source": [
        "def evaluate_model(model, X_train, y_train):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    evaluate_on_training_set(y_test, y_pred)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEqjOG9coTgn"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOZbFBAwoXM7"
      },
      "source": [
        "## Logistic Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYKrSdttkVWr"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def find_best_logreg_model(power, X_train, y_train):\n",
        "    best_f1_model = None\n",
        "    best_f1 = -1\n",
        "    best_recall = -1\n",
        "    best_i = 0\n",
        "    \n",
        "    for i in range(power + 1):\n",
        "        model = LogisticRegression(n_jobs=3, C=10**i)\n",
        "        \n",
        "        f1_scores, recall_scores, model_kcross = k_cross(model, X_train, y_train)\n",
        "        f1 = np.mean(f1_scores)\n",
        "        recall = np.mean(recall_scores)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_recall = recall\n",
        "            best_f1_model = model\n",
        "            best_i = i \n",
        "    \n",
        "    print(f\"best scores: f1 = {best_f1}, recall = {best_recall}, i = {best_i}\")\n",
        "    \n",
        "    return best_f1_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYIPoxCTkH_u",
        "outputId": "d080024f-7d7b-438e-fc75-9eaeed45f8d3"
      },
      "source": [
        "best_logreg_f1_model = find_best_logreg_model(6, X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best scores: f1 = 0.6642156862745099, recall = 0.6642156862745099, i = 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLGxZ_iDkH_u",
        "outputId": "0c4b2629-dd0a-4793-f31d-41f5abd8341a"
      },
      "source": [
        "best_logreg_f1_model = find_best_logreg_model(6, xu_train, yu_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best scores: f1 = 0.46249999999999997, recall = 0.46249999999999997, i = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GN8zzPWmN1j"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJiZDZSomThN"
      },
      "source": [
        "def find_best_tree_model(upper_depth, upper_leaf, X_train, y_train):\n",
        "    best_f1_model = None\n",
        "    best_f1 = -1\n",
        "    best_recall = -1\n",
        "    best_depth = 0\n",
        "    best_leaf = 0\n",
        "    \n",
        "    for depth in range(1, upper_depth + 1):\n",
        "        for leaf in range(1, upper_leaf + 1):\n",
        "            model = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=depth, min_samples_leaf=leaf) \n",
        "            \n",
        "            f1_scores, recall_scores, model = k_cross(model, X_train, y_train)\n",
        "            f1 = np.mean(f1_scores)\n",
        "            recall = np.mean(recall_scores)\n",
        "            \n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_recall = recall\n",
        "                best_f1_model = model\n",
        "                best_leaf = leaf\n",
        "                best_depth = depth \n",
        "    \n",
        "    print(f\"best scores: f1 = {best_f1}, recall = {best_recall}, best_leaf = {best_leaf}, best_depth = {best_depth}\")\n",
        "    \n",
        "    return best_f1_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDoxhFockH_u",
        "outputId": "fd33c109-67f6-4174-e768-cea3e2464118"
      },
      "source": [
        "best_tree_f1_model = find_best_tree_model(20, 30, X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best scores: f1 = 0.6985294117647061, recall = 0.698529411764706, best_leaf = 30, best_depth = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNkxVt8hkH_v",
        "outputId": "00e2a0ef-cbbe-4eb1-ccf7-d460e721b7c1"
      },
      "source": [
        "best_tree_f1_model = find_best_tree_model(20, 30, xu_train, yu_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best scores: f1 = 0.5890873015873016, recall = 0.5890873015873016, best_leaf = 22, best_depth = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29E0iEUkmYZn"
      },
      "source": [
        "## Random Forest "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUVCE0pmdk_"
      },
      "source": [
        "def find_best_forest_model(n_estimators, X_train, y_train):\n",
        "    best_f1_model = None\n",
        "    best_f1 = -1\n",
        "    best_recall = -1\n",
        "    best_estimator = 0\n",
        "    \n",
        "    for estimator in range(1, n_estimators + 1):\n",
        "        model = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=estimator) \n",
        "        \n",
        "        f1_scores, recall_scores, model = k_cross(model, X_train, y_train)\n",
        "        f1 = np.mean(f1_scores)\n",
        "        recall = np.mean(recall_scores)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_recall = recall\n",
        "            best_f1_model = model\n",
        "            best_estimator = estimator\n",
        "    \n",
        "    print(f\"best scores: f1 = {best_f1}, recall = {best_recall}, best estimator = {best_estimator}\")\n",
        "    \n",
        "    return best_f1_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qf0skLekH_v",
        "outputId": "0c35b03d-f13e-40c0-db5b-8008d64409a0"
      },
      "source": [
        "best_forest_f1_model = find_best_forest_model(30, X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best scores: f1 = 0.6789215686274509, recall = 0.6789215686274509, best estimator = 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d5afcOJkH_v",
        "outputId": "789fb4ac-ac84-4d6a-d58b-0ca5743c7e49"
      },
      "source": [
        "best_forest_f1_model = find_best_forest_model(30, xu_train, yu_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best scores: f1 = 0.5503968253968253, recall = 0.5503968253968253, best estimator = 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IdQt-FWmht2"
      },
      "source": [
        "## SVM with Grid Search "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwckgq3Imj73"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100]},\n",
        "                    {'kernel': ['poly'], 'degree': [3, 4, 5], 'C': [1, 10, 100]},\n",
        "                    {'kernel': ['linear'], 'C': [1, 10, 100]}]\n",
        "\n",
        "svm_model_cv = GridSearchCV(SVC(), tuned_parameters, cv=10, scoring='f1', verbose=1, n_jobs=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C5YScF4mNOw",
        "outputId": "f58c7a6d-23d5-4b2f-bf22-8b91fd9471bd"
      },
      "source": [
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100]},\n",
        "                    {'kernel': ['poly'], 'degree': [3, 4, 5], 'C': [1, 10, 100]},\n",
        "                    {'kernel': ['linear'], 'C': [1, 10, 100]}]\n",
        "\n",
        "svm_model_cv = GridSearchCV(SVC(), tuned_parameters, cv=10, scoring='f1', verbose=1, n_jobs=4)\n",
        "\n",
        "f1_scores, recall_scores, k_model = k_cross(svm_model_cv, X_train, y_train)\n",
        "\n",
        "print(f\"f1 mean score: {np.mean(f1_scores)}\")\n",
        "print(f\"recall mean score: {np.mean(recall_scores)}\")\n",
        "\n",
        "svm_model_cv.fit(X_train, y_train)\n",
        "print(svm_model_cv.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done 150 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  2.7min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  6.9min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  5.5min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  1.2min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  2.9min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  2.0min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  5.4min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 131 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  1.8min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  7.0min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 131 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  9.4min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  2.1min finished\n",
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "f1 mean score: 0.625\n",
            "recall mean score: 0.625\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed:    2.2s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  3.5min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1GmPltCoZmm",
        "outputId": "a99fb10c-980b-4605-b097-2f5c855ba91b"
      },
      "source": [
        "print(\"UNDERSAMPLE\")\n",
        "svm_model_cv = GridSearchCV(SVC(), tuned_parameters, cv=10, scoring='f1', verbose=1, n_jobs=4)\n",
        "f1_scores, recall_scores, model_kcross = k_cross(svm_model_cv, xu_train, yu_train)\n",
        "\n",
        "print(f\"f1 mean score: {np.mean(f1_scores)}, recall score: {np.mean(recall_scores)}\")\n",
        "\n",
        "print(f\"Best SVM parameters: {best_svm_model.best_params_}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNDERSAMPLE\n",
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  4.8min finished\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed: 31.0min finished\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed: 10.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/glendawee/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-8f6f8cfea820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UNDERSAMPLE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvm_model_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuned_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf1_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kcross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_cross\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_model_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myu_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"f1 mean score: {np.mean(f1_scores)}, recall score: {np.mean(recall_scores)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-be2c0975308f>\u001b[0m in \u001b[0;36mk_cross\u001b[0;34m(input_model, X, y, k, n, random_state)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79B3jTvIkH_v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}