{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pronoun_daicwoz.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ncTfJrOLoFpS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ5YUNoro8gI"
      },
      "source": [
        "# Importing necessary modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO2lXWmrGIRI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "BLNq0B3ng3wB",
        "outputId": "5b43a510-5426-4d72-c7b2-1df8c176eb16"
      },
      "source": [
        "!pip install pycontractions\n",
        "# !pip install contractions\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# import contractions\n",
        "from pycontractions import Contractions\n",
        "import re as re\n",
        "import string\n",
        "import random  \n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Embedding, Flatten \n",
        "from keras.datasets import imdb \n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.vis_utils import plot_model\n",
        "# Metrics\n",
        "from sklearn import utils\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, \\\n",
        "        f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.base import clone\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "# Classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycontractions\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/f5/d3ec9491c530cbc03af32ca2c6b69b0e89660daeb2856b485d90f9d82e5e/pycontractions-2.0.1-py3-none-any.whl\n",
            "Collecting language-check>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/97/45/0fd1d3683d6129f30fa09143fa383cdf6dff8bc0d1648f2cf156109cb772/language-check-1.1.tar.gz\n",
            "Requirement already satisfied: gensim>=2.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (3.6.0)\n",
            "Requirement already satisfied: pyemd>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (3.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=2.0->pycontractions) (3.0.4)\n",
            "Building wheels for collected packages: language-check\n",
            "  Building wheel for language-check (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for language-check\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for language-check\n",
            "Failed to build language-check\n",
            "Installing collected packages: language-check, pycontractions\n",
            "    Running setup.py install for language-check ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-pq4sb0n0/language-check/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-pq4sb0n0/language-check/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-qbl4y4bp/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-483758011e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# import contractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycontractions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycontractions'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJjxHGvFL1Gn"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YQWjVhppB-n"
      },
      "source": [
        "# Removing stop words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saCL9NJehtfv"
      },
      "source": [
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# depressed persons tend to use first-person pronouns more, and third-person pronouns less. these words might provide indication\n",
        "excluded_pronouns = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
        "                    \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
        "                     'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']\n",
        "\n",
        "for pronoun in excluded_pronouns:\n",
        "    stop_words.remove(pronoun)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "6XIO4Ib6gPKI",
        "outputId": "e8060784-d4dc-4b16-c2cd-b4ef07461a8a"
      },
      "source": [
        "transcript_filepath = \"raw_compiled_transcripts.csv\"\n",
        "df_transcript = pd.read_csv(transcript_filepath, index_col = \"Participant_ID\")\n",
        "\n",
        "column_list = df_transcript.columns.tolist()\n",
        "print(column_list)\n",
        "\n",
        "df_transcript.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Transcript', 'PHQ_Score', 'PHQ_Binary']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Transcript</th>\n",
              "      <th>PHQ_Score</th>\n",
              "      <th>PHQ_Binary</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Participant_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>good atlanta georgia um my parents are from he...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>thank you mmm k i'm doing good thank you i'm f...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>i'm fine how about yourself  i'm from los ange...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>okay how 'bout yourself here in california yea...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>i'm doing good um from los angeles california ...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>i'm doing alright uh originally i'm from calif...</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>fine uh colorado mhm uh career career possibil...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>&lt;laughter&gt; um moscow um my family moved to the...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>los angeles california yes um the southern cal...</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>&lt;laughter&gt; &lt;laughter&gt; yeah &lt;laughter&gt; &lt;laughte...</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       Transcript  ...  PHQ_Binary\n",
              "Participant_ID                                                     ...            \n",
              "300             good atlanta georgia um my parents are from he...  ...           0\n",
              "301             thank you mmm k i'm doing good thank you i'm f...  ...           0\n",
              "302             i'm fine how about yourself  i'm from los ange...  ...           0\n",
              "303             okay how 'bout yourself here in california yea...  ...           0\n",
              "304             i'm doing good um from los angeles california ...  ...           0\n",
              "305             i'm doing alright uh originally i'm from calif...  ...           0\n",
              "306             fine uh colorado mhm uh career career possibil...  ...           0\n",
              "307             <laughter> um moscow um my family moved to the...  ...           0\n",
              "308             los angeles california yes um the southern cal...  ...           1\n",
              "309             <laughter> <laughter> yeah <laughter> <laughte...  ...           1\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKV7gZJdhRWT"
      },
      "source": [
        "def filter_text(text):\n",
        "    \n",
        "    # text = re.sub('<[^<]+?>', '', text) # remove anything enclosed by tags e.g. <>\n",
        "    # text = re.sub('\\[(.*?)\\]', '', text) # remove anything enclosed by closed brackets i.e. []\n",
        "    text = contractions.fix(text) # expands contractions e.g.'he's happy' -> 'he is happy'\n",
        "    \n",
        "    text = text.lower() # contractions will capitalize 'i'\n",
        "    # text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # remove remaining punctuations e.g. apostrophe\n",
        "    \n",
        "    tokens = TweetTokenizer().tokenize(text) # use TweetTokenizer as transcripts contain informal texts\n",
        "    \n",
        "    filtered_sentence = [w for w in tokens if w not in stop_words]\n",
        "    text = ' '.join(filtered_sentence)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "zZL05GFUhjcX",
        "outputId": "091530db-caba-4265-e031-48e44a6e9ab6"
      },
      "source": [
        "# Filter the text\n",
        "\n",
        "original_text = df_transcript.Transcript.iloc[30]\n",
        "\n",
        "df_transcript.Transcript = df_transcript.Transcript.apply(filter_text)\n",
        "filtered_text = df_transcript.Transcript.iloc[30]\n",
        "\n",
        "print(f\"The original text is:\\n{original_text}\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"The filtered text is:\\n{filtered_text}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-df53f263b4c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moriginal_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_transcript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_transcript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_transcript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfiltered_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_transcript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4211\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4212\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-cfbd0444a153>\u001b[0m in \u001b[0;36mfilter_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# text = re.sub('<[^<]+?>', '', text) # remove anything enclosed by tags e.g. <>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# text = re.sub('\\[(.*?)\\]', '', text) # remove anything enclosed by closed brackets i.e. []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# expands contractions e.g.'he's happy' -> 'he is happy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# contractions will capitalize 'i'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'contractions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcjth9ktraLl"
      },
      "source": [
        "# Counting different features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqCRuFpkym3Z"
      },
      "source": [
        "## Tagged\n",
        "We need to find actions that are tagged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-7SRt9urXp8"
      },
      "source": [
        "import re\n",
        "\n",
        "def find_angle_bracket(transcript): \n",
        "  # for word in transcript:\n",
        "    # print(word)\n",
        "    pattern = \"<(.*?)>\"\n",
        "\n",
        "    substring = re.search(pattern, transcript)\n",
        "\n",
        "    approved = [\"laughter\", \"clears throat\", \"sigh\" ,\"yawn\", \"sniff\", \"sniffle\",\n",
        "                \"cough\", \"sharp inhale\", \"sharp inhaling\", \"sneeze\", \"deep breath\"]\n",
        "\n",
        "    if substring == None:\n",
        "      return \"None\"\n",
        "    else: \n",
        "      x = substring.group(1).strip() \n",
        "      if x not in approved:\n",
        "        return \"None\"\n",
        "      else:     \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JcPcZ-Gyr1Z"
      },
      "source": [
        "def find_square_bracket(transcript): \n",
        "  pattern = \"\\[(.*?)\\]\"\n",
        "\n",
        "  substring = re.search(pattern, transcript)\n",
        "\n",
        "  #remove all spaces so all input is constant\n",
        "  approved = [\"laughter\", \"clears throat\", \"sigh\" ,\"yawn\", \"sniff\", \"sniffle\",\n",
        "              \"cough\", \"sharp inhale\", \"sharp inhaling\", \"sneeze\", \"deep breath\"]\n",
        "\n",
        "  if substring == None:\n",
        "    return \"None\"\n",
        "  else: \n",
        "    x = substring.group(1).strip() \n",
        "    # print(x)\n",
        "    if x not in approved:\n",
        "      return \"None\"\n",
        "    else:     \n",
        "      return (x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWa75_9aoCJh"
      },
      "source": [
        "## Pronouns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4q_41b9cIbG"
      },
      "source": [
        "def calculating_score(text, target_words):\n",
        "    score = 0\n",
        "    for w in text:\n",
        "        for x in target_words:\n",
        "            if w == x:\n",
        "                score += 1\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncTfJrOLoFpS"
      },
      "source": [
        "## Putting Everything Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTBuDkO2afd"
      },
      "source": [
        "# iterate through raw_compiled_scripts and add count \n",
        "\n",
        "def count_features():\n",
        "\n",
        "  # creating new dataframe output \n",
        "  columns = [\"Participant_ID\", \"Laughs\", \"Sighs\", \"Clears_Throat\", \"Yawns\", \"Sniffs\", \"Coughs\", \"Sharp_Inhales\", \"Sneezes\", \"Deep_Breaths\", \n",
        "             \"Absolute_Words\", \"First_Pronoun_Singular\", \"First_Pronoun_Plural\", \"Third_Pronoun\"]\n",
        "  df_features = pd.DataFrame(columns=columns)\n",
        "  # df_features.head()  \n",
        "\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  absolute_list = [\"complet\", \"never\", \"absolut\", \"onli\", \"everi\", \"not\" \"alway\", \"just\", \"none\"]\n",
        "  first_pronoun_s_list = [\"i\", \"me\", \"mine\", \"myself\"]\n",
        "  first_pronoun_p_list = [\"we\", \"us\"]\n",
        "  third_pronoun_list = [\"he\", \"she\", \"it\", \"they\", \"him\", \"her\", \"it\", \"your\", \"his\", \"her\"]\n",
        "\n",
        "  #iterate through all words in transcript and count features \n",
        "  for index, row in df_transcript.iterrows():\n",
        "      participant_ID = index\n",
        "      phq_Binary = row[\"PHQ_Binary\"]\n",
        "      transcript = row[\"Transcript\"]\n",
        "\n",
        "      transcript = transcript.split()\n",
        "\n",
        "      if index == 300:\n",
        "        print(transcript)\n",
        "\n",
        "      if index == 333:\n",
        "        print(transcript)\n",
        "    \n",
        "      laughs = 0\n",
        "      sighs = 0 \n",
        "      clears_throat = 0 \n",
        "      yawns = 0 \n",
        "      sniffs = 0\n",
        "      coughs = 0\n",
        "      sharp_inhales = 0 \n",
        "      sneezes = 0\n",
        "      deep_breaths = 0 \n",
        "\n",
        "      absolute_words = 0\n",
        "      first_pronouns_s = 0\n",
        "      first_pronouns_p = 0\n",
        "      third_pronouns = 0\n",
        "\n",
        "      #calculating pronouns:\n",
        "      absolute_words = calculating_score(transcript, absolute_list)\n",
        "      first_pronouns_s = calculating_score(transcript, first_pronoun_s_list)\n",
        "      first_pronouns_p = calculating_score(transcript, first_pronoun_p_list)\n",
        "      third_pronouns = calculating_score(transcript, third_pronoun_list)\n",
        "\n",
        "      for word in transcript: \n",
        "        word_angle = find_angle_bracket(word)\n",
        "        word_square = find_square_bracket(word) \n",
        "\n",
        "        # get how many actions 1 person did\n",
        "        if word_angle != \"None\" and word_square == \"None\":\n",
        "          if word_angle == \"laughter\":\n",
        "            laughs += 1\n",
        "          elif word_angle == \"clears throat\":\n",
        "            clears_throat += 1\n",
        "          elif word_angle == \"sigh\": \n",
        "            sighs += 1\n",
        "          elif word_angle == \"yawn\":\n",
        "            yawns += 1\n",
        "          elif word_angle == \"sniff\":\n",
        "            sniffs += 1\n",
        "          elif word_angle == \"sniffle\":\n",
        "            sniffs += 1\n",
        "          elif word_angle == \"cough\":\n",
        "            coughs += 1\n",
        "          elif word_angle == \"sharp inhale\":\n",
        "            sharp_inhales += 1\n",
        "          elif word_angle == \"sharp inhaling\":\n",
        "            sharp_inhales += 1\n",
        "          elif word_angle == \"sneeze\":\n",
        "            sneezes += 1\n",
        "          elif word_angle == \"deep breath\":\n",
        "            deep_breaths += 1\n",
        "        \n",
        "        elif word_square != \"None\" and word_angle == \"None\":\n",
        "          if word_square == \"laughter\":\n",
        "            laughs += 1\n",
        "          elif word_square == \"clears throat\":\n",
        "            clears_throat += 1\n",
        "          elif word_square == \"sigh\": \n",
        "            sighs += 1\n",
        "          elif word_square == \"yawn\":\n",
        "            yawns += 1\n",
        "          elif word_square == \"sniff\":\n",
        "            sniffs += 1\n",
        "          elif word_square == \"sniffle\":\n",
        "            sniffs += 1\n",
        "          elif word_square == \"cough\":\n",
        "            coughs += 1\n",
        "          elif word_square == \"sharp inhale\":\n",
        "            sharp_inhales += 1\n",
        "          elif word_square == \"sharp inhaling\":\n",
        "            sharp_inhales += 1\n",
        "          elif word_square == \"sneeze\":\n",
        "            sneezes += 1\n",
        "          elif word_square == \"deep breath\":\n",
        "            deep_breaths += 1\n",
        "\n",
        "        # counting the number of pronouns \n",
        "\n",
        "        stemmed = ps.stem(word)\n",
        "\n",
        "        # if stemmed in absolute_list: \n",
        "        \n",
        "        # if stemmed in first_pronoun_s_list:\n",
        "        #   first_pronouns_s += 1\n",
        "        # elif stemmed in first_pronoun_p_list:\n",
        "        #   first_pronouns_p += 1\n",
        "        # elif stemmed in third_pronoun: \n",
        "        #   third_pronouns += 1\n",
        "    \n",
        "      new_row = {\n",
        "                \"Participant_ID\": index,\n",
        "                \"Laughs\": laughs,\n",
        "                \"Sighs\": sighs,\n",
        "                \"Clears_Throat\": clears_throat,\n",
        "                \"Yawns\": yawns,\n",
        "                \"Sniffs\": sniffs,\n",
        "                \"Coughs\": coughs,\n",
        "                \"Sharp_Inhales\": sharp_inhales,\n",
        "                \"Sneezes\": sneezes,\n",
        "                \"Deep_Breaths\": deep_breaths,\n",
        "                \"Absolute_Words\": absolute_words,\n",
        "                \"First_Pronoun_Singular\": first_pronouns_s,\n",
        "                \"First_Pronoun_Plural\": first_pronouns_p,\n",
        "                \"Third_Pronoun\": third_pronouns,\n",
        "                # \"PHQ_Binary\": phq_Binary\n",
        "      }\n",
        "\n",
        "      # print(new_row)\n",
        "\n",
        "      df_features = df_features.append(new_row, ignore_index = True)\n",
        "  \n",
        "  return df_features\n",
        "\n",
        "df_features = count_features() \n",
        "print(df_features.columns.tolist())\n",
        "df_features.head(30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp7B1qBuoc84"
      },
      "source": [
        "# Creating Sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ebYeJ5Lohqy"
      },
      "source": [
        "## Normal Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7rqTuJ9gOlz"
      },
      "source": [
        "np_features = df_features.to_numpy()\n",
        "print(np_features.shape)\n",
        "print(np_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHO4O8CKpcIW"
      },
      "source": [
        "def train_test(X, y, testfile):\n",
        "    test_participants = pd.read_csv(testfile)['participant_ID'].values\n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "    \n",
        "    for i in range(y.shape[0]):\n",
        "        participant_no = y.index[i]\n",
        "        \n",
        "        if participant_no in test_participants:\n",
        "            X_test.append(X[i])\n",
        "            y_test.append(y[participant_no])\n",
        "        \n",
        "        else:\n",
        "            X_train.append(X[i])\n",
        "            y_train.append(y[participant_no])\n",
        "            \n",
        "    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1xqxh8apfKo"
      },
      "source": [
        "testfile = './test_split_Depression_AVEC2017 (3).csv'\n",
        "\n",
        "X = np_features\n",
        "y =  pd.read_csv(\"./raw_compiled_transcripts.csv\", index_col = \"Participant_ID\")['PHQ_Binary']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test(X, y, testfile)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79kwCiPRoi6T"
      },
      "source": [
        "## Undersample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xvLpoejhjrA"
      },
      "source": [
        "RANDOM_STATE = 42\n",
        "\n",
        "# because the RandomUnderSampler does not work here\n",
        "def undersampling(X_train, y_train):\n",
        "    random.seed(RANDOM_STATE)\n",
        "    \n",
        "    neg_list = [i for i in range(len(y_train)) if y_train[i] == 0]\n",
        "    pos_list = [i for i in range(len(y_train)) if y_train[i] == 1]\n",
        "    \n",
        "    if len(neg_list) < len(pos_list):\n",
        "        minority_list = neg_list\n",
        "        majority_list = pos_list\n",
        "    else:\n",
        "        minority_list = pos_list\n",
        "        majority_list = neg_list\n",
        "        \n",
        "    sampled_list = random.sample(majority_list, len(minority_list))\n",
        "    \n",
        "    final_list = sampled_list + minority_list\n",
        "    \n",
        "    X_train_us = []\n",
        "    y_train_us = []\n",
        "    \n",
        "    for i in final_list:\n",
        "        X_train_us.append(X_train[i])\n",
        "        y_train_us.append(y_train[i])\n",
        "                                 \n",
        "    return np.array(X_train_us), np.array(y_train_us)\n",
        "\n",
        "xu_train, yu_train = undersampling(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkQiOGfrhkdZ"
      },
      "source": [
        "# shuffle the train data in unison because data is in order\n",
        "# reduces poor performance during k-cross validation when sampling data\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "xu_train, yu_train = shuffle(X_train, y_train, random_state=RANDOM_STATE)\n",
        "\n",
        "xu_train.shape, X_test.shape, yu_train.shape, y_test.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI-FqqBhj_R9"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OthD2Hy0kC8W"
      },
      "source": [
        "def evaluate_on_training_set(y_test, y_pred):\n",
        "    print(y_pred)\n",
        "    print(y_test)\n",
        "    \n",
        "    # Calculate AUC\n",
        "    print(\"AUC is: \", roc_auc_score(y_test, y_pred))\n",
        "\n",
        "    # print out recall and precision\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # print out confusion matrix\n",
        "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # # calculate points for ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_score(y_test, y_pred))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzg2QIaPkE61"
      },
      "source": [
        "def k_cross(input_model, X=X_train, y=y_train, k=10, n=1, random_state=RANDOM_STATE):\n",
        "    f1_scores = []\n",
        "    recall_scores = []\n",
        "    rkf = RepeatedKFold(n_splits=k, n_repeats=n, random_state=RANDOM_STATE)\n",
        "        \n",
        "    for train_index, val_index in rkf.split(X):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "                \n",
        "        model = clone(input_model) # prevents incremental fitting\n",
        "        model.fit(X_train, y_train) \n",
        "        \n",
        "        y_pred = model.predict(X_val)\n",
        "        f1 = f1_score(y_val, y_pred)\n",
        "        f1_scores.append(f1)\n",
        "        recall = recall_score(y_val, y_pred)\n",
        "        recall_scores.append(recall)\n",
        "        \n",
        "    return f1_scores, recall_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uDFPYUjkLr_"
      },
      "source": [
        "## Evaluate Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXMRztI2kOI_"
      },
      "source": [
        "def evaluate_model(model):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    evaluate_on_training_set(y_test, y_pred)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEqjOG9coTgn"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOZbFBAwoXM7"
      },
      "source": [
        "## Logistic Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYKrSdttkVWr"
      },
      "source": [
        "def find_best_logreg_model(power):\n",
        "    best_f1_model = None\n",
        "    best_f1 = -1\n",
        "    best_recall = -1\n",
        "    \n",
        "    for i in range(power + 1):\n",
        "        model = LogisticRegression(n_jobs=3, C=10**i)\n",
        "        \n",
        "        f1_scores, recall_scores = k_cross(model)\n",
        "        f1 = np.mean(f1_scores)\n",
        "        recall = np.mean(recall_scores)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_recall = recall\n",
        "            best_f1_model = model\n",
        "    \n",
        "    print(f\"best scores: f1 = {best_f1}, recall = {best_recall}\")\n",
        "    \n",
        "    return best_f1_model\n",
        "\n",
        "best_logreg_f1_model = find_best_logreg_model(6)\n",
        "evaluate_model(best_logreg_f1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GN8zzPWmN1j"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJiZDZSomThN"
      },
      "source": [
        "def find_best_tree_model(upper_depth, upper_leaf):\n",
        "    best_f1_model = None\n",
        "    best_f1 = -1\n",
        "    best_recall = -1\n",
        "    \n",
        "    for depth in range(1, upper_depth + 1):\n",
        "        for leaf in range(1, upper_leaf + 1):\n",
        "            model = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=depth, min_samples_leaf=leaf) \n",
        "            \n",
        "            f1_scores, recall_scores = k_cross(model)\n",
        "            f1 = np.mean(f1_scores)\n",
        "            recall = np.mean(recall_scores)\n",
        "            \n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_recall = recall\n",
        "                best_f1_model = model\n",
        "    \n",
        "    print(f\"best scores: f1 = {best_f1}, recall = {best_recall}\")\n",
        "    \n",
        "    return best_f1_model\n",
        "\n",
        "best_tree_f1_model = find_best_tree_model(20, 30)\n",
        "evaluate_model(best_tree_f1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29E0iEUkmYZn"
      },
      "source": [
        "## Random Forest "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUVCE0pmdk_"
      },
      "source": [
        "def find_best_forest_model(n_estimators):\n",
        "    best_f1_model = None\n",
        "    best_f1 = -1\n",
        "    best_recall = -1\n",
        "    \n",
        "    for estimator in range(1, n_estimators + 1):\n",
        "        model = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=estimator) \n",
        "        \n",
        "        f1_scores, recall_scores = k_cross(model)\n",
        "        f1 = np.mean(f1_scores)\n",
        "        recall = np.mean(recall_scores)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_recall = recall\n",
        "            best_f1_model = model\n",
        "    \n",
        "    print(f\"best scores: f1 = {best_f1}, recall = {best_recall}\")\n",
        "    \n",
        "    return best_f1_model\n",
        "\n",
        "best_forest_f1_model = find_best_forest_model(30)\n",
        "evaluate_model(best_forest_f1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IdQt-FWmht2"
      },
      "source": [
        "## SVM with Grid Search "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwckgq3Imj73"
      },
      "source": [
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100]},\n",
        "                    {'kernel': ['poly'], 'degree': [3, 4, 5], 'C': [1, 10, 100]},\n",
        "                    {'kernel': ['linear'], 'C': [1, 10, 100]}]\n",
        "\n",
        "svm_model_cv = GridSearchCV(SVC(), tuned_parameters, cv=10, scoring='f1', verbose=1, n_jobs=4)\n",
        "  -\n",
        "f1_scores, recall_scores = k_cross(svm_model_cv)\n",
        "\n",
        "print(f\"f1 mean score: {np.mean(f1_scores)}\")\n",
        "print(f\"recall mean score: {np.mean(recall_scores)}\")\n",
        "\n",
        "best_svm_model = evaluate_model(svm_model_cv)\n",
        "print(f\"Best SVM parameters: {best_svm_model.best_params_}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C5YScF4mNOw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1GmPltCoZmm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}